{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "annoying-domain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from pattern.en import conjugate, lemma, lexeme,PRESENT,SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "former-elite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give\n",
      "['give', 'gives', 'giving', 'gave', 'given']\n",
      "take\n",
      "['take', 'takes', 'taking', 'took', 'taken']\n",
      "['have', 'has', 'having', 'had', \"haven't\", \"hasn't\", \"hadn't\"]\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import conjugate, lemma, lexeme, PRESENT, SG\n",
    "print (lemma('gave'))\n",
    "print (lexeme('gave'))\n",
    "print (lemma('took'))\n",
    "print (lexeme('take'))\n",
    "print (lexeme('had'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-original",
   "metadata": {},
   "source": [
    "# Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-rubber",
   "metadata": {},
   "source": [
    "## Tools to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "gross-connectivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sw_nltk = stopwords.words('english')\n",
    "print(sw_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94d7630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(textfile):\n",
    "    #put entire text file into a list of sentences\n",
    "    text = []\n",
    "    with open(textfile, \"r\") as f:\n",
    "        for line in f:\n",
    "            text = text + sent_tokenize(line)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "funny-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile1 = \"data/set4/a1.txt\"\n",
    "textfile2 = \"data/set4/a7.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "raising-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_artist, text_hp = get_text(textfile1), get_text(textfile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nuclear-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_sentence(sentence):\n",
    "    #list of sentences\n",
    "    POS_tag_dict = dict()\n",
    "    text = sentence.split()\n",
    "    for line in text:\n",
    "        tags = []\n",
    "        doc = nlp(str(line))\n",
    "        for token in doc:\n",
    "            tags.append((token.pos_, token.tag_, token.dep_, token.is_stop, ))\n",
    "        if len(tags) != 0:\n",
    "            POS_tag_dict[token.text] = tags[0]\n",
    "    return POS_tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "streaming-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "#denpendency dict \n",
    "def dependency_dict(doc):\n",
    "    out = dict()\n",
    "    root = ''\n",
    "    for token in doc:\n",
    "        out[token.text] = (token.dep_, token.head.text, token.head.pos_,[child for child in token.children])\n",
    "        if token.dep_ == \"ROOT\":\n",
    "            root = token.text\n",
    "    return out, root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "funded-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_tag_sentence(sentence):\n",
    "    doc = nlp(str(sentence))\n",
    "    NER_tag_dict = dict()\n",
    "    tags = []\n",
    "    for ent in doc.ents:\n",
    "        # print(ent.text +'-' + ent.label_ + '\\n')\n",
    "        NER_tag_dict[ent.text] = ent.label_\n",
    "    return NER_tag_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-dodge",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cardiac-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bert sentence embedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "express-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"How much did the film take?\"\n",
    "question2 = 'Is Harry Potter and the Prisoner of Azkaban a 2004 fantasy film directed by Alfonso Cuarón and distributed by Warner Bros?'\n",
    "\n",
    "question1_emb = np.array(sbert_model.encode(question1)).reshape(1, -1)\n",
    "question2_emb = np.array(sbert_model.encode(question2)).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "moving-cooperative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stop words (?), contains cardinal numbers\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "len(all_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-luxembourg",
   "metadata": {},
   "source": [
    "## Turn the entire text into sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "retired-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input: text file \n",
    "#output: a dictionary of sentence embeddings {sentence: sentence embeddings}\n",
    "\n",
    "def sentence_emb(text):\n",
    "    result = dict()\n",
    "    for sentence in text:\n",
    "        sentence_emb = np.array(sbert_model.encode(sentence)).reshape(1, -1)\n",
    "        result[sentence] = sentence_emb\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "passing-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_artist_emb, text_hp_emb = sentence_emb(text_artist), sentence_emb(text_hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-forty",
   "metadata": {},
   "source": [
    "# Identify the original sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "earlier-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input: question, text embedding dict\n",
    "def find_best_sentence(question, text_emb_dict):\n",
    "    question_emb = np.array(sbert_model.encode(question)).reshape(1, -1)\n",
    "    sim_max = 0\n",
    "    output = \"\"\n",
    "    for sentence, sentence_emb in text_emb_dict.items():\n",
    "        sim = cosine_similarity(sentence_emb, question_emb)\n",
    "        if sim > sim_max:\n",
    "            sim_max = sim\n",
    "            output = sentence\n",
    "    return output, sim_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unlikely-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Question type\n",
    "question_types = [\"Who\", \"When\", \"What\" , \"Where\",\" How many\", \"How long\", \"How much\", \"Why\"]\n",
    "auxiliary_verbs = [\"am\", \"is\", \"are\", \"was\", \"were\", \"shall\", \"do\", \"does\", \"did\",\"can\", \"could\", \"have\", \"need\", \"should\", \"will\", \"would\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "lesser-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_question_type(question):\n",
    "    #Check Question type\n",
    "    for q_type in question_types:\n",
    "        if question.startswith(q_type):\n",
    "            return q_type\n",
    "    for a_verb in auxiliary_verbs:\n",
    "        a_verb = a_verb[0].upper() + a_verb[1:]\n",
    "        if question.startswith(a_verb):\n",
    "            return a_verb\n",
    "    return \"No idea\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beautiful-spouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Is', 'How much')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_question_type(question2), check_question_type(question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mediterranean-blink",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Hollywood': 'GPE', 'between 1927 and 1932': 'DATE'},\n",
       " 'The story takes place in Hollywood, between 1927 and 1932, and focuses on the relationship of an older silent film star and a rising young actress as silent cinema falls out of fashion and is replaced by the \"talkies\".')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tag_sentence(text_artist[3]), text_artist[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "olympic-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find sentence with question type as an argument\n",
    "def find_best_k_sentence(question, text_emb_dict, k):\n",
    "    question_type = check_question_type(question)\n",
    "    question_emb = np.array(sbert_model.encode(question)).reshape(1, -1)\n",
    "    sims_dict = dict()\n",
    "    output = \"\"\n",
    "    for sentence, sentence_emb in text_emb_dict.items():\n",
    "        sim = cosine_similarity(sentence_emb, question_emb)\n",
    "        #if question type and NER matches, +1\n",
    "        if check_NER(sentence, question_type):\n",
    "            sim += 1\n",
    "        #check extra matching bonus\n",
    "        extra = NER_match(question, sentence)\n",
    "        # print(extra, sentence)\n",
    "        sims_dict[sentence] = sim + extra\n",
    "    sorted_sim = sorted(sims_dict.items(), key = lambda kv: kv[1])[::-1][:k]\n",
    "    return sorted_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "plastic-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find same NER keys \n",
    "#find same words after getting rid of stopwords\n",
    "def NER_match(question, sentence):\n",
    "    output = 0\n",
    "    question_ner = ner_tag_sentence(question)\n",
    "    sentence_ner = ner_tag_sentence(sentence)\n",
    "    #find same NER key and add 0.2 for each\n",
    "    for key in question_ner.keys():\n",
    "        # print(key)\n",
    "        all_keys = [key.lower() for key in sentence_ner.keys()]\n",
    "        if key.lower() in all_keys:\n",
    "            output += 0.2\n",
    "    #find same words and plus 0.1 for each\n",
    "    q_words = [word for word in question.split() if word.lower() not in sw_nltk]\n",
    "    s_words = [word for word in sentence.split() if word.lower() not in sw_nltk]\n",
    "    for word in q_words:\n",
    "        if word in s_words:\n",
    "            output += 0.1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eligible-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the sentence contains certain NER tags\n",
    "#When - DATE\n",
    "#Who - PERSON\n",
    "#Where\" - FAC, ORG, ORG, ORG\n",
    "#\"How many\" - CARDINAL\n",
    "#\"How long\" - DATE\n",
    "#\"How much\" - MONEY\n",
    "#\"Why\" - \"because\"\n",
    "\n",
    "def check_NER(sentence, question_type):\n",
    "    output = False\n",
    "    ner_tag_dict = ner_tag_sentence(sentence)\n",
    "    if question_type == \"When\":\n",
    "        if (\"TIME\" in ner_tag_dict.values()) or (\"DATE\" in ner_tag_dict.values()):\n",
    "            output = True\n",
    "    elif question_type == \"Who\":\n",
    "        if (\"PERSON\" in ner_tag_dict.values()):\n",
    "            output = True\n",
    "    elif question_type == \"Where\":  \n",
    "        if (\"FAC\" in ner_tag_dict.values()) or (\"ORG\" in ner_tag_dict.values()) or (\"ORG\" in ner_tag_dict.values()) or (\"LOC\" in ner_tag_dict.values()):\n",
    "            output = True\n",
    "    elif question_type == \"How much\":\n",
    "        if (\"MONEY\" in ner_tag_dict.values()):\n",
    "            output = True\n",
    "    elif question_type == \"How long\":\n",
    "        if (\"DATE\" in ner_tag_dict.values()):\n",
    "            output = True\n",
    "    elif question_type == \"How many\":\n",
    "        if (\"CARDINAL\" in ner_tag_dict.values()):\n",
    "            output = True\n",
    "    elif question_type == \"How often\":\n",
    "        if (\"CARDINAL\" in ner_tag_dict.values()):\n",
    "            output = True\n",
    "    elif question_type == \"Why\":\n",
    "        if (\"because\" in sentence) or (\"due to\" in sentence) or (\"Due to\" in sentence):\n",
    "            output = True\n",
    "    else:\n",
    "        for aux in auxiliary_verbs:\n",
    "            aux_cap = aux[0].upper() + aux[1:]\n",
    "            if aux or aux_cap in sentence:\n",
    "                output = True\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "expected-recorder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_NER(text_artist[3], 'How many')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "relative-stuff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Artist (film)', array([[0.64464533]], dtype=float32))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1, sim_max1 = find_best_sentence(question1, text_artist_emb)\n",
    "output1, sim_max1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "korean-potential",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Harry Potter and the Prisoner of Azkaban is a 2004 fantasy film directed by Alfonso Cuarón and distributed by Warner Bros. Pictures.',\n",
       " array([[0.9836956]], dtype=float32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2, sim_max2 = find_best_sentence(question2, text_hp_emb)\n",
    "output2, sim_max2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "under-japan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Artist grossed $44,671,682 in North America, along with $88,761,174 in other territories for a worldwide total of $133,432,856.',\n",
       "  array([[1.279254]], dtype=float32)),\n",
       " ('The Artist (film)', array([[0.64464533]], dtype=float32)),\n",
       " ('The film was produced by La Petite Reine and ARP Sélection for 13.47 million euro, including co-production support from Studio 37 and France 3 Cinéma, and pre-sales investment from Canal+ and CinéCinéma.',\n",
       "  array([[0.58277726]], dtype=float32)),\n",
       " ('To recreate the slightly sped-up look of 1920s silent films, the film was shot at a slightly lower frame rate of 22 fps as opposed to the standard 24 fps.',\n",
       "  array([[0.55374616]], dtype=float32)),\n",
       " ('In response, director Hazanavicius released a statement:',\n",
       "  array([[0.5327408]], dtype=float32)),\n",
       " ('The film was initially given limited release in the United States on 23 November 2011.',\n",
       "  array([[0.5283947]], dtype=float32)),\n",
       " ('Box officeEdit', array([[0.5197315]], dtype=float32)),\n",
       " ('ProductionEdit', array([[0.5167223]], dtype=float32)),\n",
       " (\"Now Valentin's only chance of avoiding bankruptcy is for his film to be a hit.\",\n",
       "  array([[0.51657176]], dtype=float32)),\n",
       " ('It was inspired by the work of Hitchcock, Lang, Ford, Lubitsch, Murnau and Wilder.',\n",
       "  array([[0.508533]], dtype=float32))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_k_sentence(question1, text_artist_emb, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "official-stock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Harry Potter and the Prisoner of Azkaban is a 2004 fantasy film directed by Alfonso Cuarón and distributed by Warner Bros. Pictures.',\n",
       "  array([[3.9836955]], dtype=float32)),\n",
       " ('Harry Potter and the Prisoner of Azkaban (film)',\n",
       "  array([[2.5757165]], dtype=float32)),\n",
       " ('Harry Potter and the Prisoner of Azkaban at Box Office Mojo',\n",
       "  array([[2.5321522]], dtype=float32)),\n",
       " ('Harry Potter and the Prisoner of Azkaban at AllMovie',\n",
       "  array([[2.5223794]], dtype=float32)),\n",
       " ('Harry Potter and the Prisoner of Azkaban at the Internet Movie Database',\n",
       "  array([[2.5186093]], dtype=float32)),\n",
       " ('Harry Potter and the Prisoner of Azkaban was nominated for Best Original Music Score (John Williams) and Best Visual Effects at the 77th Academy Awards held in 2005.',\n",
       "  array([[2.5171807]], dtype=float32)),\n",
       " ('Harry Potter and the Prisoner of Azkaban at Metacritic',\n",
       "  array([[2.4705005]], dtype=float32)),\n",
       " ('Prisoner of Azkaban earned notable critical acclaim, garnering a 91% \"Certified Fresh\" approval rating at the review aggregator Rotten Tomatoes with a consensus stating,\"Under the assured direction of Alfonso Cuaron, Harry Potter and the Prisoner of Azkaban triumphantly strikes a delicate balance between technical wizardry and complex storytelling.\"',\n",
       "  array([[2.4664173]], dtype=float32)),\n",
       " ('Harry Potter and the Prisoner of Azkaban at Rotten Tomatoes',\n",
       "  array([[2.4123685]], dtype=float32)),\n",
       " ('Harry Potter and the Prisoner of Azkaban held its New York premiere at Radio City Music Hall on 23 May 2004, followed by its London premiere at Leicester Square on 30 May 2004.',\n",
       "  array([[2.3906064]], dtype=float32))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_k_sentence(question2, text_hp_emb, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-candle",
   "metadata": {},
   "source": [
    "# BINARY Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "improved-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary Questions\n",
    "#Strip the punctuation at the end\n",
    "#input: question and original sentence in text\n",
    "#check : 1) negation words: no/not/'nt √ \n",
    "#        2) Adjectives -> check antonymn\n",
    "#        3) Check Info matching?\n",
    "\n",
    "def binary_answer(question, text_emb):\n",
    "    negate = False\n",
    "    output = \"\"\n",
    "    neg_words = {'no', 'not', \"don't\", \"doesn't\", \"did't\", \"haven't\", \"hasn't\", \"wasn't\", \"weren't\"}\n",
    "    sentence = find_best_k_sentence(question, text_emb, 1)[0][0]\n",
    "    sentence_set = set([x.lower() for x in sentence.split()])\n",
    "    question_set = set([x.lower() for x in question[:-1].split()])\n",
    "    intersect_words = sentence_set.intersection(question_set)\n",
    "    leftover_question = question_set - intersect_words\n",
    "    leftover_sentence = sentence_set - intersect_words\n",
    "    # print(\"leftover words: \")\n",
    "    # print(leftover_question, leftover_question)\n",
    "    \n",
    "    negate = not check_negate(leftover_question, leftover_question)\n",
    "    if negate:\n",
    "        #No\n",
    "        output += \"No. \" + sentence\n",
    "    else:\n",
    "        #Yes\n",
    "        output += \"Yes. \" + sentence\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "going-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neg_words = ['no', 'not',\"n't\"]\n",
    "#return true if same\n",
    "def check_negate(set1, set2):\n",
    "    # print(\"hi\")\n",
    "    negate1, negate2 = True, True\n",
    "    # print(\"Negate\", negate1, negate2)\n",
    "    if len(set1) == 0 and len(set2) == 0:\n",
    "        return negate1 and negate2\n",
    "    for item1 in set1:\n",
    "        if (item1 == 'no') or (item1 == 'not') or (\"n't\" in item1):\n",
    "            negate1 = not negate1\n",
    "            # print(\"1\", item1)\n",
    "\n",
    "    for item2 in set2:\n",
    "        if (item2 == 'no') or (item2 == 'not') or (\"n't\" in item2):\n",
    "            negate2 = not negate2\n",
    "            # print(\"2\", item2)\n",
    "    # print(\"Negate\", negate1 and negate2)\n",
    "            \n",
    "    return negate1 and negate2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-impact",
   "metadata": {},
   "source": [
    "# Get Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "chicken-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question, text_emb):\n",
    "    k = 1\n",
    "    best_k_sentence = find_best_k_sentence(question, text_emb, k)\n",
    "    top_sentence = best_k_sentence[0]\n",
    "    return top_sentence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae5c87e",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e4d93b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes. Harry Potter and the Prisoner of Azkaban is a 2004 fantasy film directed by Alfonso Cuarón and distributed by Warner Bros. Pictures.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes. Harry Potter and the Prisoner of Azkaban is a 2004 fantasy film directed by Alfonso Cuarón and distributed by Warner Bros. Pictures.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary\n",
    "question1 = \"Is Harry Potter and the Prisoner of Azkaban a a 2004 fantasy film directed by Alfonso Cuarón and distributed by Warner Bros?\"\n",
    "answer1 = binary_answer(question1, text_hp_emb)\n",
    "print(answer1)\n",
    "split_sentence(answer1, question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b218428d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter, now aged 13, has been spending another dissatisfying summer at Privet Drive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Harry Potter, now aged 13, has been spending another dissatisfying summer at Privet Drive.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Who\n",
    "question2 = \"Who has been spending another unhappy summer at Privet Drive?\"\n",
    "answer2 = get_answer(question2, text_hp_emb)\n",
    "print(answer2)\n",
    "split_sentence(answer2, question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4f587f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The film was released on 31 May 2004 in the United Kingdom and on 4 June 2004 in North America, as the first Harry Potter film released into IMAX theatres and to be using IMAX Technology.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The film was released on 31 May 2004 in the United Kingdom and on 4 June 2004 in North America.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When\n",
    "question3 = \"When was the film released in United Kingdom?\"\n",
    "answer3 = get_answer(question3, text_hp_emb)\n",
    "print(answer3)\n",
    "split_sentence(answer3, question3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1082217e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fat Lady's portrait, which guards the Gryffindor quarters, is found ruined and empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The Fat Lady's portrait, which guards the Gryffindor quarters, is found ruined and empty.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What\n",
    "question4 = \"What is found ruined and empty?\"\n",
    "answer4 = get_answer(question4, text_hp_emb)\n",
    "print(answer4)\n",
    "split_sentence(answer4, question4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "382adcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Knight Bus delivers Harry to the Leaky Cauldron, where he is forgiven by Minister of Magic Cornelius Fudge for using magic outside of Hogwarts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Harry is forgiven by Minister of Magic Cornelius Fudge for using magic outside of Hogwarts at the Leaky Cauldron.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Where\n",
    "question5 = \"Where is Harry forgiven by Minister of Magic Cornelius Fudge for using magic outside of Hogwarts?\"\n",
    "answer5 = get_answer(question5, text_hp_emb)\n",
    "print(answer5)\n",
    "split_sentence(answer5, question5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "palestinian-remark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The film was released on 31 May 2004 in the United Kingdom and on 4 June 2004 in North America, as the first Harry Potter film released into IMAX theatres and to be using IMAX Technology.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The film was released on 31 May 2004 in the United Kingdom and on 4 June 2004 in North America.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Where\n",
    "question5 = \"Where was the film released on 31 May 2004?\"\n",
    "answer5 = get_answer(question5, text_hp_emb)\n",
    "print(answer5)\n",
    "split_sentence(answer5, question5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76ab69fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oldman accepted the part because he needed the money, as he had not taken on any major work in several years, having decided to spend more time with his children.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Oldman accepted the part because he needed the money.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why\n",
    "question6 = \"Why did Oldman accept the part?\"\n",
    "answer6 = get_answer(question6, text_hp_emb)\n",
    "print(answer6)\n",
    "split_sentence(answer6, question6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5838a0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prisoner of Azkaban grossed a total of $796.7 million worldwide, with its box office performance ranking as the lowest-grossing in the series.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Prisoner of Azkaban grossed a total of $796.7 million worldwide.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much\n",
    "question7 = \"How much did the Prisoner of Azkaban grossed a total of worldwide?\"\n",
    "get_answer(question7, text_hp_emb)\n",
    "answer7 = get_answer(question7, text_hp_emb)\n",
    "print(answer7)\n",
    "split_sentence(answer7, question7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "098670c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The film was nominated for two Academy Awards, Best Original Music Score and Best Visual Effects at the 77th Academy Awards in 2005.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The film was nominated for two Academy Awards.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many\n",
    "question8 = \"How many Academy Awards was the film nominated for?\"\n",
    "get_answer(question8, text_hp_emb)\n",
    "answer8 = get_answer(question8, text_hp_emb)\n",
    "print(answer8)\n",
    "split_sentence(answer8, question8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b5c45f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four months after Harris's death, Cuarón chose Gambon as his replacement.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Four months after Harris's death, Cuarón chose Gambon as his replacement.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How long\n",
    "question9 = \"How long after Harris's death, did Cuaron choose Gambon as his replacement?\"\n",
    "answer9 = get_answer(question9, text_hp_emb)\n",
    "print(answer9)\n",
    "split_sentence(answer9, question9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-serbia",
   "metadata": {},
   "source": [
    "# Split sentence and get the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fae756d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relativeWhoClause(answer, question):\n",
    "    ner_dict = ner_tag_sentence(nlp(answer.split(\",\")[0]+','))\n",
    "    if ner_dict == {}: return answer\n",
    "    output = list(ner_dict.keys())[-1]\n",
    "    if ner_dict[output] not in [\"PERSON\"]:\n",
    "        return answer\n",
    "    else:\n",
    "        question = question[:-1]\n",
    "        question_list = question.split(\" \")\n",
    "        question_list.pop(0)\n",
    "        question_list.insert(0,output)\n",
    "        return \" \".join(question_list) + \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4b9f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relativeWhenClause(answer, question):\n",
    "    ner_dict = ner_tag_sentence(nlp(answer.split(\",\")[0]+','))\n",
    "    if ner_dict == {}: return answer\n",
    "    output = list(ner_dict.keys())[-1]\n",
    "    if ner_dict[output] not in [\"DATE\", \"TIME\"]: return answer\n",
    "    else:\n",
    "        question_list = question.split(\" \")\n",
    "        if question_list[1] in [\"is\", \"are\", \"was\", \"were\"]:\n",
    "            curr = question_list[1]\n",
    "            root = dependency_dict(nlp(question))[1]\n",
    "            if root in [\"is\", \"are\", \"was\", \"were\"]: return answer\n",
    "            question = question[:-1]\n",
    "            question_list = question.split(\" \")\n",
    "            root_index = question_list.index(root)\n",
    "            question_list.insert(root_index, curr)\n",
    "            question_list.pop(0)\n",
    "            question_list.pop(0)\n",
    "            return \" \".join(question_list) + \" at \" + output + \".\"\n",
    "        elif question_list[1] in [\"do\", \"does\", \"did\"]:\n",
    "            if question_list[1] == \"does\":\n",
    "                root = dependency_dict(nlp(question))[1]\n",
    "                root_index = question_list.index(root)\n",
    "                new_root = lexeme(root)[1]\n",
    "                question_list[root_index] = new_root\n",
    "                question_list.pop(0)\n",
    "                question_list.pop(0)\n",
    "                return \" \".join(question_list) + \" at \" + output + \".\"\n",
    "            elif question_list[1] == \"do\":\n",
    "                question_list.pop(0)\n",
    "                question_list.pop(0)\n",
    "                return \" \".join(question_list) + \" at \" + output + \".\"\n",
    "            else:\n",
    "                root = dependency_dict(nlp(question))[1]\n",
    "                root_index = question_list.index(root)\n",
    "                new_root = lexeme(root)[3]\n",
    "                question_list[root_index] = new_root\n",
    "                question_list.pop(0)\n",
    "                question_list.pop(0)\n",
    "                return \" \".join(question_list) + \" at \" + output + \".\"\n",
    "        else:\n",
    "            return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1d0f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relativeWhereClause(answer, question):\n",
    "    ner_dict = ner_tag_sentence(nlp(answer.split(\",\")[0]+','))\n",
    "    output = list(ner_dict.keys())[-1]\n",
    "    if ner_dict[output] not in [\"FAC\", \"ORG\", \"GPE\", \"LOC\"]:\n",
    "        return answer\n",
    "    else:\n",
    "        question = question[:-1]\n",
    "        question_list = question.split(\" \")\n",
    "        if question_list[1] in [\"is\", \"are\", \"was\", \"were\"]:\n",
    "            curr = question_list[1]\n",
    "            root = dependency_dict(nlp(question))[1]\n",
    "            root_index = question_list.index(root)\n",
    "            question_list.insert(root_index, curr)\n",
    "            question_list.pop(0)\n",
    "            question_list.pop(0)\n",
    "            return \" \".join(question_list) + \" at \" + output + \".\"\n",
    "        elif question_list[1] in [\"do\", \"does\", \"did\"]:\n",
    "            if question_list[1] == \"does\":\n",
    "                root = dependency_dict(nlp(question))[1]\n",
    "                root_index = question_list.index(root)\n",
    "                new_root = lexeme(root)[1]\n",
    "                question_list[root_index] = new_root\n",
    "                question_list.pop(0)\n",
    "                question_list.pop(0)\n",
    "                return \" \".join(question_list) + \" at \" + output + \".\"\n",
    "            elif question_list[1] == \"do\":\n",
    "                question_list.pop(0)\n",
    "                question_list.pop(0)\n",
    "                return \" \".join(question_list) + \" at \" + output + \".\"\n",
    "            else:\n",
    "                root = dependency_dict(nlp(question))[1]\n",
    "                root_index = question_list.index(root)\n",
    "                new_root = lexeme(root)[3]\n",
    "                question_list[root_index] = new_root\n",
    "                question_list.pop(0)\n",
    "                question_list.pop(0)\n",
    "                return \" \".join(question_list) + \" at \" + output + \".\"\n",
    "        else:\n",
    "            return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "american-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(answer, question):\n",
    "    question_emb = np.array(sbert_model.encode(question)).reshape(1, -1)\n",
    "    answer_emb = np.array(sbert_model.encode(answer)).reshape(1, -1)\n",
    "    overall_sim = cosine_similarity(answer_emb, question_emb)\n",
    "    max_sim = 0\n",
    "    max_part = None\n",
    "    for sentence in answer.split(','):\n",
    "#         print(sentence)\n",
    "        if sentence.startswith(' '): \n",
    "            sentence = sentence[1:]\n",
    "#         print(sentence)\n",
    "        sentence_emb = np.array(sbert_model.encode(sentence)).reshape(1, -1)\n",
    "        curr_sim = cosine_similarity(sentence_emb, question_emb)\n",
    "#         print(curr_sim)\n",
    "        if curr_sim > max_sim and check_complete_sentence(sentence):\n",
    "            max_sim = curr_sim\n",
    "            max_part = sentence\n",
    "    if max_part == None or overall_sim > max_sim:\n",
    "        max_part = answer\n",
    "    if max_part[-1] != '.':\n",
    "        max_part += '.'\n",
    "    if max_part.startswith(\"who\"):\n",
    "        max_part = relativeWhoClause(answer, question)\n",
    "    elif max_part.startswith(\"where\"):\n",
    "        max_part = relativeWhereClause(answer, question)\n",
    "    elif max_part.startswith(\"when\"):\n",
    "        max_part = relativeWhenClause(answer, question)\n",
    "    return max_part[0].upper() + max_part[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "mexican-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_complete_sentence(sentence):\n",
    "    complete = False\n",
    "    dependence_dict, root = dependency_dict(nlp(sentence))\n",
    "#     print(dependence_dict)\n",
    "    for item in dependence_dict.values():\n",
    "        if (item[0] == 'nsubj' or item[0] == 'nsubjpass'):\n",
    "            complete = True\n",
    "    return complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebdf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-tourism",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-investment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7092a2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36] *",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "textfile = \"data/set4/a7.txt\"\n",
    "#put entire text file into a list of sentences\n",
    "def get_text(textFile):\n",
    "\ttext = []\n",
    "\twith open(textFile, \"r\") as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\ttext = text + sent_tokenize(line)\n",
    "\treturn text\n",
    "text = get_text(textfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [('h', 'X', 'LS', 'ROOT', False)], 1: [('e', 'VERB', 'VB', 'ROOT', False)], 2: [('l', 'NOUN', 'NN', 'ROOT', False)], 3: [('l', 'NOUN', 'NN', 'ROOT', False)], 4: [('o', 'INTJ', 'UH', 'ROOT', False)], 5: [(' ', 'SPACE', '_SP', 'ROOT', False)], 6: [('m', 'VERB', 'VBP', 'ROOT', False)], 7: [('y', 'PROPN', 'NNP', 'ROOT', False)], 8: [(' ', 'SPACE', '_SP', 'ROOT', False)], 9: [('n', 'ADV', 'RB', 'ROOT', False)], 10: [('a', 'DET', 'DT', 'ROOT', True)], 11: [('m', 'VERB', 'VBP', 'ROOT', False)], 12: [('e', 'VERB', 'VB', 'ROOT', False)], 13: [(' ', 'SPACE', '_SP', 'ROOT', False)], 14: [('i', 'PRON', 'PRP', 'ROOT', True)], 15: [('s', 'VERB', 'VBZ', 'ROOT', False)], 16: [(' ', 'SPACE', '_SP', 'ROOT', False)], 17: [('L', 'NOUN', 'NN', 'ROOT', False)], 18: [('a', 'DET', 'DT', 'ROOT', True)], 19: [('u', 'NOUN', 'NN', 'ROOT', False)], 20: [('r', 'NOUN', 'NN', 'ROOT', False)], 21: [('a', 'DET', 'DT', 'ROOT', True)]}\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag_lst(\"hello my name is Laura\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-3-deafc25b4eb5>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-deafc25b4eb5>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    print(doc)\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "def pos_tag_lst(text):\n",
    "\t#list of sentences\n",
    "\tPOS_tag_dict = dict()\n",
    "\tfor i,line in enumerate(text):\n",
    "\t\ttags = []\n",
    "\t\tdoc = nlp(str(line))\n",
    "        print(doc)\n",
    "\t\tfor token in doc:\n",
    "#             print(doc)\n",
    "#             print(token)\n",
    "\t\t\ttags.append((token.text, token.pos_, token.tag_, token.dep_, token.is_stop))\n",
    "\t\tif len(tags) != 0:\n",
    "\t\t\tPOS_tag_dict[i] = tagsb\n",
    "\treturn POS_tag_dict\n",
    "\n",
    "def pos_tag_sentence(sentence):\n",
    "\t#list of sentences\n",
    "\tPOS_tag_dict = dict()\n",
    "\ttext = sentence.split()\n",
    "\tfor line in text:\n",
    "\t\ttags = []\n",
    "\t\tdoc = nlp(str(line))\n",
    "\t\tfor token in doc:\n",
    "\t\t\ttags.append((token.pos_, token.tag_, token.dep_, token.is_stop, ))\n",
    "\t\tif len(tags) != 0:\n",
    "\t\t\tPOS_tag_dict[token.text] = tags[0]\n",
    "\treturn POS_tag_dict\n",
    "\n",
    "#Token dict\n",
    "def dependency_dict(doc):\n",
    "\tout = dict()\n",
    "\troot = ''\n",
    "\tfor token in doc:\n",
    "\t\tout[token.text] = (token.dep_, token.head.text, token.head.pos_,[child for child in token.children])\n",
    "\t\tif token.dep_ == \"ROOT\":\n",
    "\t\t\troot = token.text\n",
    "\treturn out, root\n",
    "\n",
    "# NER tagging\n",
    "def ner_tag(text):\n",
    "\tNER_tag_dict = dict()\n",
    "\tfor i,line in enumerate(text):\n",
    "\t\ttags = []\n",
    "\t\tdoc = nlp(str(line))\n",
    "\n",
    "\t\tfor ent in doc.ents:\n",
    "\t\t\t# print(ent.text +'-' + ent.label_ + '\\n')\n",
    "\t\t\ttags.append(ent.text +'-' + ent.label_)\n",
    "\t\tif len(tags) != 0:\n",
    "\t\t\tNER_tag_dict[i] = tags\n",
    "\treturn NER_tag_dict\n",
    "\n",
    "def ner_tag_sentence(sentence):\n",
    "\tdoc = nlp(str(sentence))\n",
    "\tNER_tag_dict = dict()\n",
    "\ttags = []\n",
    "\tfor ent in doc.ents:\n",
    "\t\t# print(ent.text +'-' + ent.label_ + '\\n')\n",
    "\t\tNER_tag_dict[ent.text] = ent.label_\n",
    "\treturn NER_tag_dict\n",
    "\n",
    "\n",
    "#check tense of verb\n",
    "def check_tense(root, pos_dict):\n",
    "\ttag = pos_dict[root][1]\n",
    "\tif tag == \"VB\":\n",
    "\t\treturn \"do\"\n",
    "\telif tag == \"VBD\":\n",
    "\t\treturn \"did\"\n",
    "\telif tag == \"VBG\":\n",
    "\t\treturn \"doing\"\n",
    "\telif tag == \"VBN\":\n",
    "\t\treturn \"done\"\n",
    "\telif tag == \"VBP\":\n",
    "\t\treturn \"do\"\n",
    "\telif tag == \"VBZ\":\n",
    "\t\treturn \"does\"\n",
    "\telse:\n",
    "\t\treturn None\n",
    "\n",
    "# return dictionary of lemmas in sentence\n",
    "def getTokenLemma(nlp_doc):\n",
    "\tlemmas = {}\n",
    "\tfor token in nlp_doc:\n",
    "\t\tlemmas[str(token)] = token.lemma_\n",
    "\treturn lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return dictionary of lemmas in sentence\n",
    "def getTokenLemma(nlp_doc):\n",
    "\tlemmas = {}\n",
    "\tfor token in nlp_doc:\n",
    "\t\tlemmas[str(token)] = token.lemma_\n",
    "\treturn lemmas\n",
    "\n",
    "#check tense of verb\n",
    "def check_tense(root, pos_dict):\n",
    "\ttag = pos_dict[root][1]\n",
    "\tif tag == \"VB\":\n",
    "\t\treturn \"do\"\n",
    "\telif tag == \"VBD\":\n",
    "\t\treturn \"did\"\n",
    "\telif tag == \"VBG\":\n",
    "\t\treturn \"doing\"\n",
    "\telif tag == \"VBN\":\n",
    "\t\treturn \"done\"\n",
    "\telif tag == \"VBP\":\n",
    "\t\treturn \"do\"\n",
    "\telif tag == \"VBZ\":\n",
    "\t\treturn \"does\"\n",
    "\telse:\n",
    "\t\treturn None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pos_tag_sentence(sentence):\n",
    "#     #list of sentences\n",
    "#     POS_tag_dict = dict()\n",
    "#     text = sentence.split()\n",
    "#     for i,line in enumerate(text):\n",
    "#         tags = []\n",
    "#         doc = nlp(str(line))\n",
    "#         for token in doc:\n",
    "#             tags.append((token.text, token.pos_, token.tag_, token.dep_, token.is_stop, ))\n",
    "#         if len(tags) != 0:\n",
    "#             POS_tag_dict[i] = tags\n",
    "#     return POS_tag_dict\n",
    "def pos_tag_sentence(sentence):\n",
    "\t# list of sentences\n",
    "\tPOS_tag_dict = dict()\n",
    "\ttext = sentence.split()\n",
    "\tfor line in text:\n",
    "\t\ttags = []\n",
    "\t\tdoc = nlp(str(line))\n",
    "\t\tfor token in doc:\n",
    "\t\t\ttags.append((token.pos_, token.tag_, token.dep_, token.is_stop,))\n",
    "\t\tif len(tags) != 0:\n",
    "\t\t\tPOS_tag_dict[token.text] = tags[0]\n",
    "\treturn POS_tag_dict\n",
    "\n",
    "def pos_tag_lst(text):\n",
    "    #list of sentences\n",
    "    POS_tag_dict = dict()\n",
    "    for i,line in enumerate(text):\n",
    "        tags = []\n",
    "        doc = nlp(str(line))\n",
    "        for token in doc:\n",
    "            tags.append((token.text, token.pos_, token.tag_, token.dep_, token.is_stop))\n",
    "        if len(tags) != 0:\n",
    "            POS_tag_dict[i] = tags\n",
    "    return POS_tag_dict\n",
    "\n",
    "#Token dict \n",
    "def dependency_dict(doc):\n",
    "    out = dict()\n",
    "    root = ''\n",
    "    for token in doc:\n",
    "        out[token.text] = (token.dep_, token.head.text, token.head.pos_,[child for child in token.children])\n",
    "        if token.dep_ == \"ROOT\":\n",
    "            root = token.text\n",
    "    return out, root\n",
    "\n",
    "def ner_tag_sentence(sentence):\n",
    "    doc = nlp(str(sentence))\n",
    "    NER_tag_dict = dict()\n",
    "    tags = []\n",
    "    for ent in doc.ents:\n",
    "        # print(ent.text +'-' + ent.label_ + '\\n')\n",
    "        NER_tag_dict[ent.text] = ent.label_\n",
    "    return NER_tag_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HowOftenQ(sentence, ner_tag_dict, dependency_dict, root):\n",
    "    date = \"\"\n",
    "    output = \"\"\n",
    "    clause = \"\"\n",
    "    sentence = sentence[:-1]\n",
    "    for k in ner_tag_dict.keys():\n",
    "        if ner_tag_dict[k] == 'DATE':\n",
    "            date = k\n",
    "    if date == \"\": return\n",
    "    date_lst = date.split()\n",
    "    sentence_lst = sentence.split()\n",
    "    root_lst_ind = sentence_lst.index(root)\n",
    "    root_ind = sentence.index(root)\n",
    "    date_ind = sentence.index(date)\n",
    "    if date_ind < root_ind:\n",
    "        word_in_front_of_root = sentence_lst[root_lst_ind - 1]\n",
    "        if token_Dict[word_in_front_of_root][0] == 'auxpass':\n",
    "            sentence_lst[root_lst_ind - 1] = sentence_lst[root_lst_ind - 2]\n",
    "            sentence_lst[root_lst_ind - 2] = word_in_front_of_root\n",
    "        else:\n",
    "            sentence_lst[root_lst_ind] = nlp(root)[0].lemma_\n",
    "            if pos_tag_sentence(root)[0][0][2] in (\"VBD\", \"VBN\"):\n",
    "                sentence_lst.insert(root_lst_ind-1,\"did\")\n",
    "            else: sentence_lst.insert(root_lst_ind-1,\"does\")\n",
    "        for i in date_lst:\n",
    "            sentence_lst.remove(i)\n",
    "        output = \"How long \" + \" \".join(sentence_lst)\n",
    "    else:\n",
    "        for seg in sentence.split(',', 1):\n",
    "            if date in seg: clause = seg\n",
    "        if root_ind != 0:\n",
    "            word_in_front_of_root = sentence_lst[root_lst_ind - 1] \n",
    "            word_after_root = sentence_lst[root_lst_ind + 1]\n",
    "            prep_ind = sentence.index(word_after_root)\n",
    "            if token_Dict[word_after_root][0] != 'prep': word_after_root = \"\"\n",
    "            #if it's passive tense\n",
    "            if token_Dict[word_in_front_of_root][0] == 'auxpass':\n",
    "                output = \"How long \" + word_in_front_of_root + \" \" + clause.split(word_in_front_of_root,1)[0].lower() +  sentence[root_ind:prep_ind+len(word_after_root)]\n",
    "            else:\n",
    "                #if it's past tense\n",
    "                if pos_tag_sentence(root)[0][0][2] in (\"VBD\", \"VBN\"):\n",
    "                    output = \"How long\" + \" did \" + clause.split(root,1)[0].lower() + nlp(root)[0].lemma_ + sentence[root_ind+len(root):prep_ind+len(word_after_root)]\n",
    "                #if it's not past tense\n",
    "                else:\n",
    "                    output = \"How long\" + \" does \" + clause.split(root,1)[0].lower() + nlp(root)[0].lemma_ + sentence[root_ind+len(root):prep_ind+len(word_after_root)]\n",
    "    output = ' '.join(output.split()) + \"?\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Four months after Harris's death, Cuarón chose Gambon as his replacement.\"\n",
    "#sentence = \"With Prisoner of Azkaban, production of the Harry Potter films was switched to an eighteen-month cycle.\"\n",
    "# sentence = \"Harry Potter and the Prisoner of Azkaban is a 2004 fantasy film directed by Alfonso Cuaron and distributed by Warner Bros. Pictures.\"\n",
    "doc = nlp(sentence)\n",
    "token_Dict, root = dependency_dict(doc)\n",
    "ner_tag_dict = ner_tag_sentence(sentence)\n",
    "pos_tag_dict = pos_tag_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Four': ('NUM', 'CD', 'ROOT', True),\n",
       " 'months': ('NOUN', 'NNS', 'ROOT', False),\n",
       " 'after': ('ADP', 'IN', 'ROOT', True),\n",
       " \"'s\": ('PROPN', 'NNP', 'ROOT', False),\n",
       " ',': ('NOUN', 'NN', 'ROOT', False),\n",
       " 'Cuarón': ('PROPN', 'NNP', 'ROOT', False),\n",
       " 'chose': ('VERB', 'VBD', 'ROOT', False),\n",
       " 'Gambon': ('PROPN', 'NNP', 'ROOT', False),\n",
       " 'as': ('ADP', 'IN', 'ROOT', True),\n",
       " 'his': ('PRON', 'PRP$', 'ROOT', True),\n",
       " '.': ('NOUN', 'NN', 'ROOT', False)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [('is', 'AUX', 'VBZ', 'ROOT', True)]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_sentence(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [('Four', 'NUM', 'CD', 'ROOT', True)],\n",
       " 1: [('months', 'NOUN', 'NNS', 'ROOT', False)],\n",
       " 2: [('after', 'ADP', 'IN', 'ROOT', True)],\n",
       " 3: [('Harris', 'PROPN', 'NNP', 'ROOT', False),\n",
       "  (\"'s\", 'PART', 'POS', 'case', True)],\n",
       " 4: [('death', 'NOUN', 'NN', 'ROOT', False),\n",
       "  (',', 'PUNCT', ',', 'punct', False)],\n",
       " 5: [('Cuarón', 'PROPN', 'NNP', 'ROOT', False)],\n",
       " 6: [('chose', 'VERB', 'VBD', 'ROOT', False)],\n",
       " 7: [('Gambon', 'PROPN', 'NNP', 'ROOT', False)],\n",
       " 8: [('as', 'ADP', 'IN', 'ROOT', True)],\n",
       " 9: [('his', 'PRON', 'PRP$', 'ROOT', True)],\n",
       " 10: [('replacement', 'NOUN', 'NN', 'ROOT', False),\n",
       "  ('.', 'PUNCT', '.', 'punct', False)]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How long does harry potter and the prbe?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HowOftenQ(sentence, ner_tag_dict, token_Dict, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whereQ(sentence, dep_dict, root):\n",
    "    output = ''\n",
    "    verbs = ['was', 'is', 'were']\n",
    "    foundVerb = False\n",
    "    foundVerbInd = 0\n",
    "    # find tense \n",
    "    pos_tags = pos_tag_sentence(sentence)\n",
    "    seenWhere = False\n",
    "    foundWhereInd = 0\n",
    "    whereInd = 0\n",
    "    tense = None\n",
    "    for word in sentence.split():\n",
    "        if word == 'where':\n",
    "            seenWhere = True\n",
    "            foundWhereInd = whereInd\n",
    "            verb = dep_dict[word][1]\n",
    "            if pos_tags[verb][0] == 'VERB':\n",
    "                tense = check_tense(verb, pos_tags)\n",
    "            elif verb in verbs: # if tense is was, is, were\n",
    "                tense = verb\n",
    "                foundVerb = True\n",
    "                foundVerbInd = sentence.split().index(verb)\n",
    "            break\n",
    "        whereInd += 1\n",
    "    if tense == None: # not able to create question from sentence\n",
    "        return \"No question\"\n",
    "    output += f'Where {tense} '\n",
    "    ind = foundWhereInd\n",
    "    if foundVerb:\n",
    "        ind = foundVerbInd\n",
    "    for word in sentence.split()[ind+1:]:\n",
    "        output += word + \" \"    \n",
    "    return output[:-2] + \"?\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': ('nsubj', 'leads', 'VERB', []), 'leads': ('ROOT', 'leads', 'VERB', [This, trio, to, .]), 'the': ('det', 'dog', 'NOUN', []), 'trio': ('dobj', 'leads', 'VERB', [the]), 'to': ('prep', 'passage', 'NOUN', [Shack]), 'an': ('det', 'Animagus', 'PROPN', []), 'underground': ('amod', 'passage', 'NOUN', []), 'passage': ('pobj', 'to', 'ADP', [an, underground, to]), 'Shrieking': ('compound', 'Shack', 'PROPN', []), 'Shack': ('pobj', 'to', 'ADP', [the, Shrieking, ,, discover]), ',': ('punct', 'Sirius', 'PROPN', []), 'where': ('advmod', 'discover', 'VERB', []), 'they': ('nsubj', 'discover', 'VERB', []), 'discover': ('relcl', 'Shack', 'PROPN', [where, they, is]), 'that': ('mark', 'is', 'AUX', []), 'dog': ('nsubj', 'is', 'AUX', [the]), 'is': ('relcl', 'Sirius', 'PROPN', [who, Animagus]), 'actually': ('advmod', 'is', 'AUX', []), 'Sirius': ('attr', 'is', 'AUX', [,, is]), 'who': ('nsubj', 'is', 'VERB', []), 'Animagus': ('attr', 'is', 'VERB', [an]), '.': ('punct', 'leads', 'VERB', [])} leads\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This leads the trio to an underground passage to the Shrieking Shack, where they discover that the dog is actually Sirius, who is an Animagus.\"\n",
    "dep_dict1, root1 = dependency_dict(nlp(sentence))\n",
    "print(dep_dict1, root1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'He': ('nsubj', 'was', 'AUX', []), 'was': ('ROOT', 'was', 'AUX', [He, in, .]), 'in': ('prep', 'was', 'AUX', [underground]), 'the': ('det', 'underground', 'NOUN', []), 'underground': ('pobj', 'in', 'ADP', [the, ,, were]), ',': ('punct', 'underground', 'NOUN', []), 'where': ('advmod', 'were', 'VERB', []), 'there': ('expl', 'were', 'VERB', []), 'were': ('relcl', 'underground', 'NOUN', [where, there, lots]), 'lots': ('attr', 'were', 'VERB', [of]), 'of': ('prep', 'lots', 'NOUN', [tea]), 'tea': ('pobj', 'of', 'ADP', []), '.': ('punct', 'was', 'AUX', [])} was\n"
     ]
    }
   ],
   "source": [
    "sentence2 = \"He was in the underground, where there were lots of tea.\"\n",
    "dep_dict2, root2 = dependency_dict(nlp(sentence2))\n",
    "print(dep_dict2, root2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'He': ('PRON', 'PRP', 'ROOT', True),\n",
       " 'was': ('AUX', 'VBD', 'ROOT', True),\n",
       " 'in': ('ADP', 'IN', 'ROOT', True),\n",
       " 'the': ('DET', 'DT', 'ROOT', True),\n",
       " ',': ('ADV', 'RB', 'ROOT', False),\n",
       " 'where': ('ADV', 'WRB', 'ROOT', True),\n",
       " 'there': ('ADV', 'RB', 'ROOT', True),\n",
       " 'were': ('AUX', 'VBD', 'ROOT', True),\n",
       " 'lots': ('NOUN', 'NNS', 'ROOT', False),\n",
       " 'of': ('ADP', 'IN', 'ROOT', True),\n",
       " '.': ('NOUN', 'NN', 'ROOT', False)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_sentence(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': ('DET', 'DT', 'ROOT', True),\n",
       " 'leads': ('VERB', 'VBZ', 'ROOT', False),\n",
       " 'the': ('DET', 'DT', 'ROOT', True),\n",
       " 'trio': ('NOUN', 'NN', 'ROOT', False),\n",
       " 'to': ('PART', 'TO', 'ROOT', True),\n",
       " 'an': ('DET', 'DT', 'ROOT', True),\n",
       " 'underground': ('ADV', 'RB', 'ROOT', False),\n",
       " 'passage': ('NOUN', 'NN', 'ROOT', False),\n",
       " 'Shrieking': ('VERB', 'VBG', 'ROOT', False),\n",
       " ',': ('PROPN', 'NNP', 'ROOT', False),\n",
       " 'where': ('ADV', 'WRB', 'ROOT', True),\n",
       " 'they': ('PRON', 'PRP', 'ROOT', True),\n",
       " 'discover': ('VERB', 'VB', 'ROOT', False),\n",
       " 'that': ('DET', 'DT', 'ROOT', True),\n",
       " 'dog': ('NOUN', 'NN', 'ROOT', False),\n",
       " 'is': ('AUX', 'VBZ', 'ROOT', True),\n",
       " 'actually': ('ADV', 'RB', 'ROOT', False),\n",
       " 'who': ('PRON', 'WP', 'ROOT', True),\n",
       " '.': ('PROPN', 'NNP', 'ROOT', False)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where do they discover that the dog is actually Sirius, who is an Animagus?\n"
     ]
    }
   ],
   "source": [
    "print(whereQ(sentence, dep_dict1, root1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where were lots of tea?\n"
     ]
    }
   ],
   "source": [
    "print(whereQ(sentence2, dep_dict2, root2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Asking- Checking Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the sentence contains certain NER tags\n",
    "#When - DATE\n",
    "#Who - PERSON\n",
    "#Where\" - FAC, ORG, ORG, ORG, GPE\n",
    "#\"How many\" - CARDINAL\n",
    "#\"How long\" - DATE\n",
    "#\"How much\" - MONEY\n",
    "#\"Why\" - \"because\"\n",
    "\n",
    "def check_NER(sentence, question_type):\n",
    "    output = False\n",
    "    ner_tag_dict = ner_tag_sentence(sentence)\n",
    "    if question_type == \"When\":\n",
    "        if (\"TIME\" in ner_tag_dict.values()) or (\"DATE\" in ner_tag_dict.values()):\n",
    "            output = True\n",
    "    elif question_type == \"Who\":\n",
    "        if (\"PERSON\" in ner_tag_dict.values()):\n",
    "            output = True\n",
    "    elif question_type == \"Where\":  \n",
    "        if (\"FAC\" in ner_tag_dict.values()) or (\"ORG\" in ner_tag_dict.values()) or (\"LOC\" in ner_tag_dict.values()):\n",
    "            output = True\n",
    "    elif question_type == \"How much\":\n",
    "        if (\"MONEY\" in ner_tag_dict.values()):\n",
    "            output = True\n",
    "    elif question_type == \"How long\":\n",
    "        if (\"DATE\" in ner_tag_dict.values()):\n",
    "            output = True\n",
    "    elif question_type == \"How many\":\n",
    "        if (\"CARDINAL\" in ner_tag_dict.values()):\n",
    "            output = True\n",
    "    elif question_type == \"How often\":\n",
    "        if (\"CARDINAL\" in ner_tag_dict.values()):\n",
    "            output = True\n",
    "    elif question_type == \"Why\":\n",
    "        if (\"because\" in sentence) or (\"due to\" in sentence) or (\"Due to\" in sentence):\n",
    "            output = True\n",
    "    else:\n",
    "        for aux in auxiliary_verbs:\n",
    "            aux_cap = aux[0].upper() + aux[1:]\n",
    "            if aux or aux_cap in sentence:\n",
    "                output = True\n",
    "    return output\n",
    "\n",
    "def checkSentenceType(sentence, dep_dict, ner_tag_dict, root):\n",
    "    possible_types = set()\n",
    "    if (\"TIME\" in ner_tag_dict.values()) or (\"DATE\" in ner_tag_dict.values()):\n",
    "        possible_types.add(\"When\")\n",
    "    if (\"PERSON\" in ner_tag_dict.values()):\n",
    "        possible_types.add(\"Who\")\n",
    "    if (\"FAC\" in ner_tag_dict.values()) or (\"ORG\" in ner_tag_dict.values()) or (\"LOC\" in ner_tag_dict.values()) or (\"GPE\" in ner_tag_dict.values()):\n",
    "        possible_types.add(\"Where\")\n",
    "    if (\"MONEY\" in ner_tag_dict.values()):\n",
    "        possible_types.add(\"How much\")\n",
    "    if (\"DATE\" in ner_tag_dict.values()):\n",
    "        possible_types.add(\"How long\")\n",
    "    if (\"CARDINAL\" in ner_tag_dict.values()):\n",
    "        possible_types.add(\"How many\")\n",
    "        possible_types.add(\"How often\")\n",
    "    if (\"because\" in sentence) or (\"due to\" in sentence) or (\"Due to\" in sentence) or (\"since\" in sentence):\n",
    "        possible_types.add(\"Why\")\n",
    "    aux_verbs = {\"are\", \"is\", \"was\", \"were\", \"shall\", \"do\", \"does\", \"did\", \n",
    "                 \"can\", \"could\", \"have\", \"need\", \"should\", \"will\", \"would\",\n",
    "                 \"must\", \"may\", \"might\", \"cannot\"}\n",
    "    if root in aux_verbs:\n",
    "        possible_types.add(\"What\")\n",
    "    for dep in dep_dict.values():\n",
    "        if dep[0] == 'nsubj':\n",
    "            possible_types.add(\"What\")\n",
    "    return possible_types\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"It is based on the novel of the same name by J. K. Rowling.\"\n",
    "# sentence = \"The story takes place in Hollywood, between 1927 and 1932, and focuses on the relationship of an older silent film star and a rising young actress as silent cinema falls out of fashion and is replaced by the 'talkies'.\"\n",
    "# sentence = \"The film, which is the third instalment in the Harry Potter film series, was written by Steve Kloves and produced by Chris Columbus (director of the first two instalments), David Heyman, and Mark Radcliffe.\"\n",
    "# sentence = \"The film stars Daniel Radcliffe as Harry Potter, alongside Rupert Grint and Emma Watson as Harry's best friends Ron Weasley and Hermione Granger.\"\n",
    "sentence = \"However, it was, at the time, the best-reviewed film of the series and currently the second-best-reviewed film of the franchise according to review aggregator Rotten Tomatoes.\"\n",
    "nlp_doc = nlp(sentence)\n",
    "dep_dict, root = dependency_dict(nlp_doc)\n",
    "ner_tag_dict = ner_tag_sentence(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'However': ('advmod', 'was', 'AUX', []),\n",
       " ',': ('punct', 'was', 'AUX', []),\n",
       " 'it': ('nsubj', 'was', 'AUX', []),\n",
       " 'was': ('ROOT', 'was', 'AUX', [However, ,, it, ,, at, ,, film, .]),\n",
       " 'at': ('prep', 'was', 'AUX', [time]),\n",
       " 'the': ('det', 'franchise', 'NOUN', []),\n",
       " 'time': ('pobj', 'at', 'ADP', [the]),\n",
       " 'best': ('advmod', 'reviewed', 'VERB', []),\n",
       " '-': ('punct', 'reviewed', 'VERB', []),\n",
       " 'reviewed': ('amod', 'film', 'NOUN', [second, -, best, -]),\n",
       " 'film': ('conj', 'film', 'NOUN', [currently, the, reviewed, of]),\n",
       " 'of': ('prep', 'film', 'NOUN', [franchise]),\n",
       " 'series': ('pobj', 'of', 'ADP', [the]),\n",
       " 'and': ('cc', 'film', 'NOUN', []),\n",
       " 'currently': ('advmod', 'film', 'NOUN', []),\n",
       " 'second': ('advmod', 'reviewed', 'VERB', []),\n",
       " 'franchise': ('pobj', 'of', 'ADP', [the]),\n",
       " 'according': ('prep', 'film', 'NOUN', [review]),\n",
       " 'to': ('aux', 'review', 'NOUN', []),\n",
       " 'review': ('pcomp', 'according', 'VERB', [to, aggregator]),\n",
       " 'aggregator': ('dobj', 'review', 'NOUN', []),\n",
       " 'Rotten': ('compound', 'Tomatoes', 'PROPN', []),\n",
       " 'Tomatoes': ('appos', 'film', 'NOUN', [Rotten]),\n",
       " '.': ('punct', 'was', 'AUX', [])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'second': 'ORDINAL', 'Rotten Tomatoes': 'PERSON'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'What', 'Who'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkSentenceType(sentence, dep_dict, ner_tag_dict, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateQuestions(object):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\ttextfile = \"test.txt\"\n",
    "\t\tself.nlp = spacy.load('en_core_web_sm')\n",
    "\t\tself.asker = Asking.Asking(textfile)\n",
    "\t\tself.parser = Parser.Parser(textfile)\n",
    "\n",
    "\tdef checkSentenceType(self, sentence, dep_dict, ner_tag_dict, root):\n",
    "\t\tpossible_types = set()\n",
    "\t\tif (\"TIME\" in ner_tag_dict.values()) or (\n",
    "\t\t\t\t\"DATE\" in ner_tag_dict.values()):\n",
    "\t\t\tpossible_types.add(\"When\")\n",
    "\t\tif (\"PERSON\" in ner_tag_dict.values()):\n",
    "\t\t\tpossible_types.add(\"Who\")\n",
    "\t\tif (\"FAC\" in ner_tag_dict.values()) or (\n",
    "\t\t\t\t\"ORG\" in ner_tag_dict.values()) or (\n",
    "\t\t\t\t\"LOC\" in ner_tag_dict.values()) or (\n",
    "\t\t\t\t\"GPE\" in ner_tag_dict.values()):\n",
    "\t\t\tpossible_types.add(\"Where\")\n",
    "\t\tif (\"MONEY\" in ner_tag_dict.values()):\n",
    "\t\t\tpossible_types.add(\"How much\")\n",
    "\t\tif (\"DATE\" in ner_tag_dict.values()):\n",
    "\t\t\tpossible_types.add(\"How long\")\n",
    "\t\tif (\"CARDINAL\" in ner_tag_dict.values()):\n",
    "\t\t\tpossible_types.add(\"How many\")\n",
    "\t\t\tpossible_types.add(\"How often\")\n",
    "\t\tif (\"because\" in sentence) or (\"due to\" in sentence) or (\n",
    "\t\t\t\t\"Due to\" in sentence) or (\"since\" in sentence):\n",
    "\t\t\tpossible_types.add(\"Why\")\n",
    "\t\taux_verbs = {\"are\", \"is\", \"was\", \"were\", \"shall\", \"do\", \"does\", \"did\",\n",
    "\t\t\t\t\t \"can\", \"could\", \"have\", \"need\", \"should\", \"will\", \"would\",\n",
    "\t\t\t\t\t \"must\", \"may\", \"might\", \"cannot\"}\n",
    "\t\tif root in aux_verbs:\n",
    "\t\t\tpossible_types.add(\"What\")\n",
    "\t\tfor dep in dep_dict.values():\n",
    "\t\t\tif dep[0] == 'nsubj':\n",
    "\t\t\t\tpossible_types.add(\"What\")\n",
    "\t\treturn possible_types\n",
    "\n",
    "\tdef generateQuestions(self, limit):\n",
    "\t\tpossible_questions = set()\n",
    "\t\tfor sentence in self.parser.text:\n",
    "\t\t\tprint(sentence)\n",
    "\t\t\tnlp_doc = self.nlp(sentence)\n",
    "\t\t\tpos_tags = self.parser.pos_tag_sentence(sentence)\n",
    "\t\t\ttoken_dict, root = self.parser.dependency_dict(nlp_doc)\n",
    "\t\t\tner_tags = self.parser.ner_tag_sentence(sentence)\n",
    "\t\t\tlemma_dict = self.parser.getTokenLemma(nlp_doc)\n",
    "\t\t\ttype_dict = {\"How many\": self.asker.howManyQ(sentence, ner_tags,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\ttoken_dict, pos_tags, root),\n",
    "\t\t\t\t\t\t \"Who\": self.asker.whoQ(sentence, ner_tags, token_dict),\n",
    "\t\t\t\t\t\t \"How much\": self.asker.howMuchQ(sentence, nlp_doc, ner_tags,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  token_dict, root, pos_tags),\n",
    "\t\t\t\t\t\t \"How often\": self.asker.howOftenQ(sentence, ner_tags, token_dict,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tpos_tags, root),\n",
    "\t\t\t\t\t\t \"Why\": self.asker.whyQ(sentence, nlp_doc, ner_tags, token_dict,\n",
    "\t\t\t\t\t\t\t\t\t\t\t pos_tags, root),\n",
    "\t\t\t\t\t\t \"Where\": self.asker.whereQ(sentence, token_dict, pos_tags),\n",
    "\t\t\t\t\t\t \"What\": self.asker.whatQ(sentence, token_dict, root, pos_tags,\n",
    "\t\t\t\t\t\t\t\t\t\t\t   lemma_dict),\n",
    "\t\t\t\t\t\t \"When\": self.asker.whenQ(sentence, ner_tags, root, nlp_doc,\n",
    "\t\t\t\t\t\t\t\t\t\t\t   pos_tags)}\n",
    "\n",
    "\t\t\t# generate question outputs\n",
    "\t\t\ttry:\n",
    "\t\t\t\tpossible_types = self.checkSentenceType(sentence, token_dict,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tner_tags, root)\n",
    "\t\t\t\tfor type in possible_types:\n",
    "\t\t\t\t\tquestion = type_dict[type]\n",
    "\t\t\t\t\tif self.isValidQuestion(question):\n",
    "\t\t\t\t\t\tprint(question)\n",
    "\t\t\t\t\t\tpossible_questions.add(question)\n",
    "\t\t\t\t\tbinary_output = self.asker.binaryQ(sentence, root)\n",
    "\t\t\t\tif self.isValidQuestion(binary_output):\n",
    "\t\t\t\t\tprint(binary_output)\n",
    "\t\t\t\t\tpossible_questions.add(binary_output)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t# limit amount of questions to be generated\n",
    "\t\t\tif len(possible_questions) >= limit:\n",
    "\t\t\t\treturn possible_questions\n",
    "\t\treturn possible_questions\n",
    "\n",
    "\t# check if generated question is valid\n",
    "\tdef isValidQuestion(self, question):\n",
    "\t\treturn question != None and len(question) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GenerateQuestions()\n",
    "generator.generateQuestions(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who Question Asking Revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: a single sentence, and its ner tag dict and dependency dict\n",
    "# Who Question\n",
    "def whoQ(sentence, ner_tag_dict, dependency_dict, root):\n",
    "    # find PERSON tag\n",
    "    theName = ''\n",
    "    output = ''\n",
    "    if root not in sentence:\n",
    "        return\n",
    "    rootInd = sentence.index(root)\n",
    "    minDist = len(sentence)\n",
    "    for ner_tag in ner_tag_dict:\n",
    "        if ner_tag_dict[ner_tag] == 'PERSON':\n",
    "            tag_ind = sentence.index(ner_tag)\n",
    "            if abs(tag_ind - rootInd) < minDist:\n",
    "                minDist = abs(tag_ind - rootInd)\n",
    "                theName = ner_tag\n",
    "    if theName == '':\n",
    "        return\n",
    "    if \"'\" in theName:\n",
    "        output = sentence.replace(theName, 'whose', 1)\n",
    "    else: \n",
    "        output = sentence.replace(theName, 'who', 1)\n",
    "    output = output[:-1] + \"?\"\n",
    "    output = output[0].upper() + output[1:]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The story follows Harry Potter's third year at Hogwarts as he is informed that a prisoner named Sirius Black has escaped from Azkaban intending to kill him.\"\n",
    "nlp_doc = nlp(sentence)\n",
    "dep_dict, root = dependency_dict(nlp_doc)\n",
    "ner_tag_dict = ner_tag_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The story follows whose third year at Hogwarts as he is informed that a prisoner named Sirius Black has escaped from Azkaban intending to kill him?'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whoQ(sentence, ner_tag_dict, dep_dict, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"The film stars Daniel Radcliffe as Harry Potter, alongside Rupert Grint and Emma Watson as Harry's best friends Ron Weasley and Hermione Granger.\"\n",
    "# sentence2 = \"It also features a star-studded supporting cast including Gary Oldman, David Thewlis, Michael Gambon (in his debut role as Albus Dumbledore), Emma Thompson and Timothy Spall.\"\n",
    "# sentence2 = \"It is the sequel to Harry Potter and the Chamber of Secrets and is followed by Harry Potter and the Goblet of Fire.\"\n",
    "nlp_doc2 = nlp(sentence2)\n",
    "dep_dict2, root2 = dependency_dict(nlp_doc2)\n",
    "ner_tag_dict2 = ner_tag_sentence(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The film stars who as Harry Potter, alongside Rupert Grint and Emma Watson as Harry's best friends Ron Weasley and Hermione Granger?\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whoQ(sentence2, ner_tag_dict2, dep_dict2, root2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Asking Revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"The film was nominated for two Academy Awards, Best Original Music Score and Best Visual Effects at the 77th Academy Awards in 2005.\"\n",
    "# sentence = \"It is also the last Harry Potter film to be released on VHS as well as the last film until Harry Potter and the Half-Blood Prince to be rated PG in North America.\"\n",
    "# sentence = \"Prisoner of Azkaban grossed a total of $796.7 million worldwide, with its box office performance ranking as the lowest-grossing in the series.\"\n",
    "# sentence = \"However, it was, at the time, the best-reviewed film of the series and currently the second-best-reviewed film of the franchise according to review aggregator Rotten Tomatoes.\"\n",
    "sentence = \"Harry Potter and the Prisoner of Azkaban is a 2004 fantasy film directed by Alfonso Cuar�n and distributed by Warner Bros. Pictures.\"\n",
    "nlp_doc = nlp(sentence)\n",
    "dep_dict, root = dependency_dict(nlp_doc)\n",
    "ner_tag_dict = ner_tag_sentence(sentence)\n",
    "pos_tag_dict = pos_tag_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Prisoner': 'Prisoner', 'of': 'of', 'Azkaban': 'Azkaban', 'grossed': 'gross', 'a': 'a', 'total': 'total', '$': '$', '796.7': '796.7', 'million': 'million', 'worldwide': 'worldwide', ',': ',', 'with': 'with', 'its': 'its', 'box': 'box', 'office': 'office', 'performance': 'performance', 'ranking': 'rank', 'as': 'as', 'the': 'the', 'lowest': 'lowest', '-': '-', 'grossing': 'gross', 'in': 'in', 'series': 'series', '.': '.'}\n"
     ]
    }
   ],
   "source": [
    "print(getTokenLemma(nlp_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Prisoner': ('PROPN', 'NNP', 'ROOT', False),\n",
       " 'of': ('ADP', 'IN', 'ROOT', True),\n",
       " 'Azkaban': ('ADJ', 'JJ', 'ROOT', False),\n",
       " 'grossed': ('VERB', 'VBN', 'ROOT', False),\n",
       " 'a': ('DET', 'DT', 'ROOT', True),\n",
       " 'total': ('ADJ', 'JJ', 'ROOT', False),\n",
       " '796.7': ('SYM', '$', 'nmod', False),\n",
       " 'million': ('NUM', 'CD', 'ROOT', False),\n",
       " ',': ('ADV', 'RB', 'ROOT', False),\n",
       " 'with': ('ADP', 'IN', 'ROOT', True),\n",
       " 'its': ('PRON', 'PRP$', 'ROOT', True),\n",
       " 'box': ('NOUN', 'NN', 'ROOT', False),\n",
       " 'office': ('NOUN', 'NN', 'ROOT', False),\n",
       " 'performance': ('NOUN', 'NN', 'ROOT', False),\n",
       " 'ranking': ('VERB', 'VBG', 'ROOT', False),\n",
       " 'as': ('ADP', 'IN', 'ROOT', True),\n",
       " 'the': ('DET', 'DT', 'ROOT', True),\n",
       " 'grossing': ('ADV', 'RBS', 'advmod', False),\n",
       " 'in': ('ADP', 'IN', 'ROOT', True),\n",
       " '.': ('PROPN', 'NNP', 'ROOT', False)}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Prisoner': ('nsubj', 'grossed', 'VERB', [of]),\n",
       " 'of': ('prep', 'total', 'NOUN', [million]),\n",
       " 'Azkaban': ('pobj', 'of', 'ADP', []),\n",
       " 'grossed': ('ROOT',\n",
       "  'grossed',\n",
       "  'VERB',\n",
       "  [Prisoner, total, worldwide, ,, with, .]),\n",
       " 'a': ('det', 'total', 'NOUN', []),\n",
       " 'total': ('dobj', 'grossed', 'VERB', [a, of]),\n",
       " '$': ('quantmod', 'million', 'NUM', []),\n",
       " '796.7': ('compound', 'million', 'NUM', []),\n",
       " 'million': ('pobj', 'of', 'ADP', [$, 796.7]),\n",
       " 'worldwide': ('advmod', 'grossed', 'VERB', []),\n",
       " ',': ('punct', 'grossed', 'VERB', []),\n",
       " 'with': ('prep', 'grossed', 'VERB', [ranking]),\n",
       " 'its': ('poss', 'performance', 'NOUN', []),\n",
       " 'box': ('compound', 'office', 'NOUN', []),\n",
       " 'office': ('compound', 'performance', 'NOUN', [box]),\n",
       " 'performance': ('nsubj', 'ranking', 'VERB', [its, office]),\n",
       " 'ranking': ('pcomp', 'with', 'ADP', [performance, as]),\n",
       " 'as': ('prep', 'ranking', 'VERB', [grossing]),\n",
       " 'the': ('det', 'series', 'NOUN', []),\n",
       " 'lowest': ('advmod', 'grossing', 'VERB', []),\n",
       " '-': ('punct', 'grossing', 'VERB', []),\n",
       " 'grossing': ('pobj', 'as', 'ADP', [the, lowest, -, in]),\n",
       " 'in': ('prep', 'grossing', 'VERB', [series]),\n",
       " 'series': ('pobj', 'in', 'ADP', [the]),\n",
       " '.': ('punct', 'grossed', 'VERB', [])}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: a single sentence, with its dependency dict and root word\n",
    "auxiliary_verbs = [\"am\", \"is\", \"are\", \"was\", \"were\", \"shall\", \"do\", \"does\", \"did\",\"can\", \"could\", \"have\", \"need\",\n",
    "                   \"should\", \"will\", \"would\"]\n",
    "def binaryQ(sentence, dep_dict, ner_tag_dict, pos_tag_dict, nlp_doc, root):\n",
    "    output = ''\n",
    "    seenRoot = False\n",
    "    if root not in sentence:\n",
    "        return\n",
    "    split = sentence.split()\n",
    "    if root + \",\" in split:\n",
    "        rootInd = split.index(root + ',')\n",
    "    else:\n",
    "        rootInd = split.index(root)\n",
    "    ner_tags = set()\n",
    "    for ner_tag in ner_tag_dict:\n",
    "        split_tag = ner_tag.split()\n",
    "        for split_word in split_tag:\n",
    "            ner_tags.add(split_word)\n",
    "    if root in auxiliary_verbs:\n",
    "        output += root.capitalize() + ' '\n",
    "        for word in split[0:rootInd]:\n",
    "            if word not in ner_tags:\n",
    "                output += word.lower() + ' '\n",
    "            else:\n",
    "                output += word + ' '\n",
    "        output += \" \".join(split[rootInd + 1:])\n",
    "        output = output[:-1] + '?'\n",
    "        return output\n",
    "    else:\n",
    "        aux_verb = \"\"\n",
    "        minDist = len(split)\n",
    "        for word in split:\n",
    "            if word in auxiliary_verbs:\n",
    "                if abs(split.index(word) - rootInd) < minDist:\n",
    "                    minDist = split.index(word) - rootInd\n",
    "                    aux_verb = word\n",
    "        if aux_verb != \"\":\n",
    "            output += aux_verb.capitalize() + ' '\n",
    "            auxInd = split.index(aux_verb)\n",
    "            output += \" \".join(split[0:auxInd]) # add everything before aux verb\n",
    "            output += \" \" + root + \" \"\n",
    "            output += \" \".join(split[rootInd + 1:])\n",
    "        else:\n",
    "            # edge case where root and nearby words not in auxiliary verbs\n",
    "            tense = check_tense(root, pos_tag_dict)\n",
    "            if tense == None: return\n",
    "            output += tense.capitalize() + ' '\n",
    "            output += \" \".join(split[0:rootInd]) + ' '\n",
    "            output += getTokenLemma(nlp_doc)[root] + ' '\n",
    "            output += \" \".join(split[rootInd + 1:])\n",
    "        split_output = output.split()\n",
    "        if split_output[1] not in ner_tags:\n",
    "            output = split_output[0] + ' ' + split_output[1].lower() + ' ' + \" \".join(split_output[2:])\n",
    "        output = output[:-1] + '?'\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is Harry Potter and the Prisoner of Azkaban a 2004 fantasy film directed by Alfonso Cuar�n and distributed by Warner Bros. Pictures?'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binaryQ(sentence, dep_dict, ner_tag_dict, pos_tag_dict, nlp_doc, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Question Revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whatQ(sentence, dep_dict, root):\n",
    "    aux_verbs = {\"are\", \"is\", \"was\", \"were\", \"shall\", \"do\", \"does\", \"did\", \n",
    "                 \"can\", \"could\", \"have\", \"need\", \"should\", \"will\", \"would\", \n",
    "                 \"must\", \"may\", \"might\", \"cannot\"}\n",
    "    output = ''\n",
    "    if root in aux_verbs:\n",
    "        output += f\"What {root} \"\n",
    "        seenRoot = False\n",
    "        for n in sentence.split():\n",
    "            if n == root or n == root + ',':\n",
    "                seenRoot = True\n",
    "                continue\n",
    "            if seenRoot:\n",
    "                output += n + \" \"\n",
    "        return output[:-2] + \"?\"\n",
    "    else:\n",
    "        splitSentence = sentence.split()\n",
    "        if root in splitSentence:\n",
    "            rootInd = splitSentence.index(root)\n",
    "        elif root + ',' in splitSentence:\n",
    "            rootInd = splitSentence.index(root + ',')\n",
    "        else:\n",
    "            return\n",
    "        if rootInd != 0:\n",
    "            if splitSentence[rootInd - 1] in aux_verbs:\n",
    "                output += f'What {splitSentence[rootInd - 1]} {root} '\n",
    "                output += \" \".join(splitSentence[rootInd + 1:])\n",
    "                return output[:-1] + \"?\"\n",
    "            elif rootInd >= 2 and splitSentence[rootInd - 1] == 'be' and splitSentence[rootInd - 2] in aux_verbs:\n",
    "                output += 'What '\n",
    "                output += \" \".join(splitSentence[rootInd - 2:])\n",
    "                return output[:-1] + \"?\"\n",
    "            else:\n",
    "                # case where nsubj is before the root\n",
    "                for word in splitSentence[:rootInd + 1]:\n",
    "                    if word in dep_dict and dep_dict[word][0] == 'nsubj':\n",
    "                        output += 'What '\n",
    "                        output += \" \".join(splitSentence[rootInd:])\n",
    "                        return output[:-1] + \"?\"\n",
    "        return        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The film stars Daniel Radcliffe as Harry Potter, alongside Rupert Grint and Emma Watson as Harry's best friends Ron Weasley and Hermione Granger.\"\n",
    "sentence = \"Prisoner of Azkaban grossed a total of $796.7 million worldwide, with its box office performance ranking as the lowest-grossing in the series.\"\n",
    "sentence = \"The film would be nominated for two Academy Awards, Best Original Music Score and Best Visual Effects at the 77th Academy Awards in 2005.\"\n",
    "# sentence = \"However, it was, at the time, the best-reviewed film of the series and currently the second-best-reviewed film of the franchise according to review aggregator Rotten Tomatoes.\"\n",
    "# sentence = \"The film was released on 31 May 2004 in the United Kingdom and on 4 June 2004 in North America, as the first Harry Potter film released into IMAX theatres and to be using IMAX Technology.\"\n",
    "nlp_doc = nlp(sentence)\n",
    "dep_dict, root = dependency_dict(nlp_doc)\n",
    "ner_tag_dict = ner_tag_sentence(sentence)\n",
    "pos_tag_dict = pos_tag_sentence(sentence)\n",
    "lemmas = getTokenLemma(nlp_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The': ('det', 'film', 'NOUN', []),\n",
       " 'film': ('nsubjpass', 'nominated', 'VERB', [The]),\n",
       " 'would': ('aux', 'nominated', 'VERB', []),\n",
       " 'be': ('auxpass', 'nominated', 'VERB', []),\n",
       " 'nominated': ('ROOT', 'nominated', 'VERB', [film, would, be, for, in, .]),\n",
       " 'for': ('prep', 'nominated', 'VERB', [Awards]),\n",
       " 'two': ('nummod', 'Awards', 'PROPN', []),\n",
       " 'Academy': ('compound', 'Awards', 'PROPN', []),\n",
       " 'Awards': ('pobj', 'at', 'ADP', [the, 77th, Academy]),\n",
       " ',': ('punct', 'Awards', 'PROPN', []),\n",
       " 'Best': ('conj', 'Score', 'PROPN', []),\n",
       " 'Original': ('compound', 'Score', 'PROPN', []),\n",
       " 'Music': ('compound', 'Score', 'PROPN', []),\n",
       " 'Score': ('nmod', 'Effects', 'NOUN', [Best, Original, Music, and, Best]),\n",
       " 'and': ('cc', 'Score', 'PROPN', []),\n",
       " 'Visual': ('compound', 'Effects', 'NOUN', []),\n",
       " 'Effects': ('appos', 'Awards', 'PROPN', [Score, Visual, at]),\n",
       " 'at': ('prep', 'Effects', 'NOUN', [Awards]),\n",
       " 'the': ('det', 'Awards', 'PROPN', []),\n",
       " '77th': ('compound', 'Awards', 'PROPN', []),\n",
       " 'in': ('prep', 'nominated', 'VERB', [2005]),\n",
       " '2005': ('pobj', 'in', 'ADP', []),\n",
       " '.': ('punct', 'nominated', 'VERB', [])}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases:\n",
    "# Q: What grossed a total of $796.7 million worldwide....? root pos = VBN\n",
    "# Q: What stars Daniel Radcliffe as Harry Potter, alongside ....?\n",
    "# Q: What was nominated for two Academy Awards,...? root pos = VBN\n",
    "# Q: What was released on 31 May 2004...?\n",
    "# Process:\n",
    "    # What + root + everything after\n",
    "    # if auxiliary verb is right before root, add auxiliary verb in front of root\n",
    "    # check if there's a 'be' before root and auxiliary verb before 'be' \n",
    "        #(if so, it should be What + aux verb + 'be' + root + everything else)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What would be nominated for two Academy Awards, Best Original Music Score and Best Visual Effects at the 77th Academy Awards in 2005?'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whatQ(sentence, dep_dict, root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
